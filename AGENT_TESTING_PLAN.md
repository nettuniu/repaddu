# AGENT TESTING PLAN for REPADDU

## Objective
Validate that the Markdown output generated by `repaddu` is optimized for Large Language Model (LLM) comprehension, retention, and reasoning. We will use a multi-agent approach to simulate "Developer" and "QA" roles.

## Methodology
We will treat the `repaddu` output as the "Knowledge Base" for an AI agent. We will then test the agent's ability to answer questions about the codebase based *solely* on that input.

## Test Environment
1.  **Subject Repository**: A "Golden Repo" containing:
    - Multiple languages (C++, Python, CMake).
    - Known bugs.
    - Hidden "Secrets" (for redaction testing).
    - Complex dependencies.
    - Binary files and large assets.
2.  **Agents**:
    - **Agent A (Generator)**: Uses `repaddu` with specific configurations to generate Markdown.
    - **Agent B (Questioner)**: Has access to the *original* source code (ground truth). Generates questions.
    - **Agent C (Answerer)**: Has access *only* to the `repaddu` Markdown output. Answers questions.
    - **Agent D (Judge)**: Compares Agent C's answers against Agent B's ground truth.

## Test Scenarios & Configurations

### Scenario 1: "Default Settings" Baseline
- **Config**: Default CLI options.
- **Goal**: Establish a baseline score for retrieval accuracy.
- **Metrics**: % of questions answered correctly.

### Scenario 2: "Deep Context" (Analysis Features)
- **Config**: `--analyze-only=false --include-dependency-graph --extract-todos --token-count`.
- **Goal**: Determine if added metadata (deps, todos) improves answers regarding architecture and technical debt.

### Scenario 3: "Noise Reduction" (Filtering)
- **Config**: `--exclude-binaries --max-size 10KB --group-by component`.
- **Goal**: Verify if removing noise (binaries, massive data files) reduces hallucinations.

### Scenario 4: "Safety Check" (PII/Secrets)
- **Config**: `--redact-pii --warn-secrets`.
- **Goal**:
    - **Positive**: Ensure secrets are NOT present in output.
    - **Negative**: Ensure the agent knows *where* a secret was (e.g., "The API key was redacted in file X").

### Scenario 5: "Chunking Stress Test"
- **Config**: `--max-bytes 500` (Extremely small chunks).
- **Goal**: Test if the "Read 000 first" and linking strategy allows the agent to reconstruct context across many files.

## Execution Workflow (Automated)

1.  **Setup**: Clone "Golden Repo".
2.  **Generate**: Run `repaddu` with Target Configuration.
3.  **Ingest**: Load generated Markdown into a RAG system or Long-Context Window (e.g., Gemini 1.5 Pro).
4.  **Quiz**:
    - Q1: "What is the primary function of `src/main.cpp`?"
    - Q2: "List all dependencies of the `Network` module."
    - Q3: "Are there any hardcoded API keys?" (Expectation: "I see a redacted placeholder...")
    - Q4: "Summarize the TODOs in the project."
5.  **Evaluate**: Score the responses (0-100).

## Deliverables
- **`report_card.md`**: A matrix showing which `repaddu` configurations yielded the highest agent accuracy.
- **`recommended_presets.yaml`**: The optimal `repaddu` config derived from testing results.
